{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Data Diagnostics II </b> *✲ﾟ*｡✧٩(･ิᴗ･ิ๑)۶*✲ﾟ*｡✧\n",
    "\n",
    "In this notebook we will explore taking the min-max or percentile normalization between datasets and also derivatives and see how our data changes, i.e. the distribution of each variable, check the principal components, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions as hf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pynumdiff as pdiff\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.covariance import MinCovDet\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.stats import mstats, boxcox\n",
    "\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "importlib.reload(hf)\n",
    "\n",
    "truncated_dataframe = pd.read_pickle('truncated_dataframe.pkl')\n",
    "dataframes = hf.wbstruct_dataframes.loading_pkl('dataframes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_num = 3329\n",
    "length_dict = defaultdict()\n",
    "for key in dataframes.keys():\n",
    "    length_dict[key] = frames_num\n",
    "turn_vec =hf.get_behavioural_states(truncated_dataframe) # this step has to be revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization between datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above we have to deal with different scales across datasets so a natural next step is to normalize the data across datasets to make them comparable. We will do this by taking the min-max normalization between datasets. This means that we will take the minimum and maximum value of each variable across all datasets and then normalize each dataset to this range. This will be done the time derivatives of the resampled data.\n",
    "\n",
    "We can also try the percentile normalization between datasets. This means that we will take the 5th and 95th percentile of each variable across all datasets and then normalize each dataset to this range. This will be done the time derivatives of the resampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw trace of AVAR across all datasets\n",
    "# plotting the trace of AVAR across all normalized datasets\n",
    "fig, ax = plt.subplots(figsize=(40, 10))\n",
    "ax.plot(truncated_dataframe['AVAR'].T, color=\"tab:blue\")\n",
    "ax.set_ylabel(\"AVAR\")\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.set_title(\"AVAR across all datasets\")\n",
    "fig.savefig(\"raw_AVAR_alldatasets.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on Quantiles: RobustScaler\n",
    "\n",
    "### Individual Datasets And Then Combined with 0.05, 0.95 Quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per dataset normalization\n",
    "scaler = RobustScaler(with_centering=False, with_scaling=True, quantile_range=(10, 90))\n",
    "quartiled_separate = hf.normalize_per_dataset(truncated_dataframe, length_dict, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# across dataset normalization\n",
    "scaler = RobustScaler(with_centering=False, with_scaling=True, quantile_range=(10, 90))\n",
    "quartiled_data = pd.DataFrame(scaler.fit_transform(quartiled_separate), columns = quartiled_separate.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncated_dataframe_new = quartiled_data.copy()\n",
    "# nested_col = [[name] * dataset_len for name, dataset_len in length_dict.items()]\n",
    "# truncated_dataframe_new['dataset'] = [x for xs in nested_col for x in xs]\n",
    "# truncated_dataframe_new.groupby('dataset')['AVAR'].hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the trace of AVAR across all normalized datasets\n",
    "fig, ax = plt.subplots(figsize=(40, 10))\n",
    "ax.plot(quartiled_data['AVAR'].T, color=\"tab:blue\")\n",
    "ax.set_ylabel(\"AVAR\")\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.set_title(\"AVAR across all datasets\")\n",
    "fig.savefig(\"normalized_AVAR_alldatasets.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_quartile = hf.PCA(n_components=3)\n",
    "imputed_pc_quartile = pd.DataFrame(pca_quartile.fit_transform(quartiled_data))\n",
    "\n",
    "window_size = 10\n",
    "\n",
    "# Applying a 10-sample sliding average for smoother visualizations!\n",
    "for i in range(3):\n",
    "    imputed_pc_quartile[i] = np.convolve(imputed_pc_quartile[i], np.ones(window_size)/window_size, mode='same')\n",
    "imputed_pc_quartile['state'] = turn_vec.values\n",
    "hf.plot_PCs(imputed_pc_quartile,imputed_pc_quartile['state'],'PCA_quartiled.html')\n",
    "#hf.plot_PC_gif(imputed_pc_quartile,imputed_pc_quartile['state'],'PCA_quartiled.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on MinMax: \n",
    "\n",
    "### Individual Datasets And Then Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmaxing per dataset\n",
    "scaler = MinMaxScaler()\n",
    "minmax_separate = hf.normalize_per_dataset(truncated_dataframe, length_dict, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmaxing across dataset\n",
    "minmax_data = pd.DataFrame(scaler.fit_transform(minmax_separate), columns = minmax_separate.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the trace of AVAR across all normalized datasets\n",
    "fig, ax = plt.subplots(figsize=(40, 10))\n",
    "ax.plot(minmax_data['AVAR'].T, color=\"tab:blue\")\n",
    "ax.set_ylabel(\"AVAR\")\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.set_title(\"AVAR across all datasets\")\n",
    "fig.savefig(\"minmaxed_AVAR_alldatasets.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_quartile = hf.PCA(n_components=3)\n",
    "imputed_pc_quartile = pd.DataFrame(pca_quartile.fit_transform(minmax_data))\n",
    "\n",
    "for i in range(3):\n",
    "    imputed_pc_quartile[i] = np.convolve(imputed_pc_quartile[i], np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "imputed_pc_quartile['state'] = turn_vec.values\n",
    "hf.plot_PCs(imputed_pc_quartile,imputed_pc_quartile['state'],'PCA_quartiled.html')\n",
    "#hf.plot_PC_gif(imputed_pc_quartile,imputed_pc_quartile['state'],'PCA_quartiled.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on Standard Deviation: QuantileTransformer\n",
    "\n",
    "### Individual Datasets And Then Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmaxing per dataset\n",
    "scaler = QuantileTransformer()\n",
    "quantile_separate = hf.normalize_per_dataset(truncated_dataframe, length_dict, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmaxing across dataset\n",
    "quantile_transformed_data = pd.DataFrame(scaler.fit_transform(quantile_separate), columns = quantile_separate.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the trace of AVAR across all normalized datasets\n",
    "fig, ax = plt.subplots(figsize=(40, 10))\n",
    "ax.plot(quantile_transformed_data['AVAR'].T, color=\"tab:blue\")\n",
    "ax.set_ylabel(\"AVAR\")\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.set_title(\"AVAR across all datasets\")\n",
    "fig.savefig(\"quantile_AVAR_alldatasets.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_quartile = hf.PCA(n_components=3)\n",
    "imputed_pc_quartile = pd.DataFrame(pca_quartile.fit_transform(quantile_transformed_data))\n",
    "\n",
    "for i in range(3):\n",
    "    imputed_pc_quartile[i] = np.convolve(imputed_pc_quartile[i], np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "imputed_pc_quartile['state'] = turn_vec.values\n",
    "hf.plot_PCs(imputed_pc_quartile,imputed_pc_quartile['state'],'PCA_quantile_transformed.html')\n",
    "#hf.plot_PC_gif(imputed_pc_quartile,imputed_pc_quartile['state'],'PCA_quartiled.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "feature_names2 = truncated_dataframe.columns\n",
    "for i in range(25):\n",
    "    end_index = (i+1) * frames_num\n",
    "    start_index = end_index - frames_num\n",
    "    fig, axes = plt.subplots(nrows=15, ncols=5, figsize=(12, 23))\n",
    "    fig.suptitle('Distribution of Some Features', y=1.02)\n",
    "\n",
    "    for n, ax in enumerate(axes.flat):\n",
    "        if n < len(feature_names2):\n",
    "            sns.histplot(truncated_dataframe[start_index:end_index], x=feature_names2[n], ax=ax)\n",
    "            ax.set(title=f'Histogram of {feature_names2[n]}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(f'histograms/{list(dataframes.keys())[i]}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
