{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Diagnostics: Variability of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from neumand.thesis_figures.variability_visualization_tools import biplot, silhouette_plots\n",
    "import helper_functions as hf\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_validate, cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.calibration import cross_val_predict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = pd.read_hdf('time_embedded_2103.h5')\n",
    "unpreprocessed_data = pd.read_hdf(\"imputed_dataframe_0602.h5\", key=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "### Individual datasets in a shared PC space (Preprocessed Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "prep_pca_data = pca.fit_transform(preprocessed_data.loc[:,~preprocessed_data.columns.isin(['state', 'dataset'])])\n",
    "prep_pca_data = pd.DataFrame(prep_pca_data)\n",
    "prep_pca_data['state'] = preprocessed_data['state']\n",
    "prep_pca_data['dataset'] = preprocessed_data['dataset']\n",
    "\n",
    "# encode the dataset column as numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_datasets = label_encoder.fit_transform(prep_pca_data['dataset'])\n",
    "prep_pca_data['dataset_numeric'] = encoded_datasets\n",
    "preprocessed_data['dataset_numeric'] = encoded_datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {dataset: df for dataset, df in prep_pca_data.groupby('dataset')}\n",
    "hf.plot_PCs_separately(datasets).run_server(debug=True, port=8054)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual datasets in a shared PC space (Unpreprocessed Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprep_pca = PCA(n_components=3)\n",
    "unprep_pca_data = unprep_pca.fit_transform(unpreprocessed_data.loc[:,~unpreprocessed_data.columns.isin(['state', 'dataset'])])\n",
    "unprep_pca_data = pd.DataFrame(unprep_pca_data)\n",
    "unprep_pca_data['state'] = unpreprocessed_data['state']\n",
    "unprep_pca_data['dataset'] = unpreprocessed_data['dataset']\n",
    "\n",
    "# encode the dataset column as numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_datasets = label_encoder.fit_transform(unprep_pca_data['dataset'])\n",
    "unprep_pca_data['dataset_numeric'] = encoded_datasets\n",
    "unpreprocessed_data['dataset_numeric'] = encoded_datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_unprep = {dataset: df for dataset, df in unprep_pca_data.groupby('dataset')}\n",
    "hf.plot_PCs_separately(datasets_unprep).run_server(debug=True, port=8055)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color-code trajectories based on dataset \n",
    "Each data point is colored based on the dataset it belongs to. This helps in understanding the variability of the data across different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_traces = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    trace = go.Scatter3d(x=df[0], y=df[1], z=df[2], mode=\"lines\", name=name)\n",
    "    all_traces.append(trace)\n",
    "    \n",
    "fig = go.Figure(data=all_traces)\n",
    "\n",
    "variances = pca.explained_variance_ratio_ * 100\n",
    "scene = dict(xaxis_title=f\"PC 1 ({variances[0]:.2f}%)\",\n",
    "                yaxis_title=f\"PC 2 ({variances[1]:.2f}%)\",\n",
    "                zaxis_title=f\"PC 3 ({variances[2]:.2f}%)\")\n",
    "\n",
    "fig.update_layout(scene=scene)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biplot within a state\n",
    "\n",
    "Biplots show us how strongly a variable influences a principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take the ventral turns\n",
    "ventral_data = preprocessed_data.loc[preprocessed_data['state']=='ventral',:]\n",
    "ventral_data.columns = ventral_data.columns.astype(str)\n",
    "pcav2 = PCA(n_components=3)\n",
    "ventral_pcs2 = pcav2.fit_transform(ventral_data.loc[:,~ventral_data.columns.isin(['state', 'dataset','dataset_numeric','cluster'])]) # exclude the dataset column\n",
    "ventral_components2 = pcav2.components_ # directions of maximum variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biplot(ventral_pcs2[:,0:3],np.transpose(pcav2.components_[0:3, :]),ventral_data['dataset_numeric'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering for dataset membership\n",
    "\n",
    "### KMeans with preprocessed data (in PC space)\n",
    "\n",
    "We can try clustering our data without the dataset feature to see if the separation of data points is based on the dataset membership or not.\n",
    "We could cross check with a dataset where no preprocessing has been done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation** <br>\n",
    "We can evaluate the clustering using the adjusted mutual information score, which calculates the mutual information between two clusterings and then normalizes this value by the expected mutual information of two random clusterings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMI = cross_val_score(KMeans(n_clusters=23),prep_pca_data.loc[:,~prep_pca_data.columns.isin(['state', 'dataset', 'dataset_numeric'])],prep_pca_data['dataset_numeric'],cv=StratifiedKFold(n_splits=5),scoring=make_scorer(adjusted_mutual_info_score))\n",
    "print(f\"AMI: {np.mean(AMI)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_pred_labels = cross_val_predict(KMeans(n_clusters=23),prep_pca_data.loc[:,~prep_pca_data.columns.isin(['state', 'dataset', 'dataset_numeric'])],prep_pca_data['dataset_numeric'],cv=StratifiedKFold(n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control: KMeans with unpreprocessed dataset (in PC space)\n",
    "We will now cluster our unpreprocessed data and see if the clusters are based on the dataset membership or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMI_unpreprocessed = cross_val_score(KMeans(n_clusters=23),unprep_pca_data.loc[:,~unprep_pca_data.columns.isin(['state', 'dataset', 'dataset_numeric'])],unprep_pca_data['dataset_numeric'],cv=StratifiedKFold(n_splits=5),scoring=make_scorer(adjusted_mutual_info_score))\n",
    "print(f\"AMI for unpreprocessed data: {np.mean(AMI_unpreprocessed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprep_pred_labels = cross_val_predict(KMeans(n_clusters=23),unprep_pca_data.loc[:,~unprep_pca_data.columns.isin(['state', 'dataset', 'dataset_numeric'])],unprep_pca_data['dataset_numeric'],cv=StratifiedKFold(n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**<br>\n",
    "Since the adjusted mutual information score between the clustering of our data and the dataset membership is relatively low (also compared to the unpreprocessed data), we can assume that the separation of data points is not based on the dataset membership and that the dataset feature might not explain the variability of the trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contingency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.clf()\n",
    "res = sns.heatmap(contingency_matrix(prep_pca_data['dataset_numeric'], prep_pred_labels), fmt='.2f', cmap=\"YlGnBu\", vmin=0.0, vmax=100.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.clf()\n",
    "res = sns.heatmap(contingency_matrix(unprep_pca_data['dataset_numeric'], unprep_pred_labels), fmt='.2f', cmap=\"YlGnBu\", vmin=0.0, vmax=100.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Coefficient\n",
    "We will also calculate the silhouette coefficient to evaluate the quality of the clusters. The silhouette score ranges from -1 to 1, where a higher value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = preprocessed_data.loc[:,~preprocessed_data.columns.isin(['state', 'dataset', 'dataset_numeric'])]\n",
    "silhouette_plots(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "We will now try to classify the preprocessed data based on the dataset membership.\n",
    "\n",
    "Note: Since cross_val_predict does not work with TimeSplit we will use custom code from stackexchange (Marco Cerliani)\n",
    "\n",
    "### State Classification on Original (Unpreprocessed) Data with TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = make_scorer(metrics.precision_score, average='weighted')\n",
    "reca = make_scorer(metrics.recall_score, average='weighted')\n",
    "f1 = make_scorer(metrics.f1_score, average='weighted')\n",
    "acc = make_scorer(metrics.accuracy_score)\n",
    "scoring={\"accuracy\":acc, \"precision\":prec, \"recall\":reca, \"f1\":f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit()\n",
    "\n",
    "X = unpreprocessed_data.loc[:,~unpreprocessed_data.columns.isin(['state', 'dataset', 'cluster'])]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y =  label_encoder.fit_transform(unpreprocessed_data['state'])\n",
    "\n",
    "tscv_results_original = cross_validate(SVC(gamma='auto'), X, y, cv=tscv, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,values in tscv_results_original.items():\n",
    "    print(name,\":\", np.mean(list(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv = StratifiedKFold()\n",
    "\n",
    "X = unpreprocessed_data.loc[:,~unpreprocessed_data.columns.isin(['state', 'dataset', 'cluster'])]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y =  label_encoder.fit_transform(unpreprocessed_data['state'])\n",
    "\n",
    "scv_results_original = cross_validate(SVC(gamma='auto'), X, y, cv=scv, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,values in scv_results_original.items():\n",
    "    print(name,\":\", np.mean(list(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Classification on Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit()\n",
    "\n",
    "X = preprocessed_data.loc[:,~preprocessed_data.columns.isin(['state', 'dataset', 'cluster','dataset_numeric'])]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y =  label_encoder.fit_transform(preprocessed_data['state'])\n",
    "\n",
    "scv_results_preprocessed = cross_validate(SVC(gamma='auto'), X, y, cv=tscv, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,values in scv_results_preprocessed.items():\n",
    "    print(name,\":\", np.mean(list(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Classification on PCA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit()\n",
    "\n",
    "X = prep_pca_data.loc[:,~prep_pca_data.columns.isin(['state', 'dataset', 'cluster', 'dataset_numeric'])]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y =  label_encoder.fit_transform(prep_pca_data['state'])\n",
    "\n",
    "cv_results_pca = cross_validate(SVC(gamma='auto'), X, y, cv=tscv, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,values in cv_results_pca.items():\n",
    "    print(name,\":\", np.mean(list(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(SVC(gamma='auto'), X, y, cv=scv, groups=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, y_pred)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=False, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Membership Classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New approach with StratifiedKFold (UNpreprocessed data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv = StratifiedKFold(n_splits=5)\n",
    "y_ds = unpreprocessed_data[\"dataset_numeric\"]\n",
    "X = unpreprocessed_data.loc[:,~unpreprocessed_data.columns.isin(['state', 'dataset', 'cluster', 'dataset_numeric'])]\n",
    "cv_results_unprep = cross_validate(SVC(gamma='auto'), X, y_ds, cv=scv, groups=y_ds, scoring=scoring)\n",
    "y_pred_unprep = cross_val_predict(SVC(gamma='auto'), X, y_ds, cv=scv, groups=y_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,values in cv_results_unprep.items():\n",
    "    print(name,\":\", np.mean(list(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_ds, y_pred_unprep)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=False, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv = StratifiedKFold(n_splits=5)\n",
    "X = preprocessed_data.loc[:,~preprocessed_data.columns.isin(['state', 'dataset', 'cluster', 'dataset_numeric'])]\n",
    "y_ds_prep = preprocessed_data[\"dataset_numeric\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_prep = cross_validate(SVC(gamma='auto'), X, y_ds_prep, cv=scv, scoring=scoring, verbose=1, error_score= 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prep = cross_val_predict(SVC(gamma='auto'), X, y_ds_prep, cv=scv, groups=y_ds_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,values in cv_results_prep.items():\n",
    "    print(name,\":\", np.mean(list(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_ds_prep, y_pred_prep)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=False, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv = StratifiedKFold(n_splits=5)\n",
    "X = prep_pca_data.loc[:,~prep_pca_data.columns.isin(['state', 'dataset', 'cluster', 'dataset_numeric'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_pca = cross_validate(SVC(gamma='auto'), X, y_ds, cv=scv, groups=y, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pca = cross_val_predict(SVC(gamma='auto'), X, y_ds, cv=scv, groups=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,values in cv_results_pca.items():\n",
    "    print(name,\":\", np.mean(list(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_ds, y_pred_pca)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=False, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = prep_pca_data['state'].unique().tolist()\n",
    "for state in states:\n",
    "    cv_results_pca = cross_validate(SVC(gamma='auto'),prep_pca_data.loc[prep_pca_data['state']==state,~prep_pca_data.columns.isin(['state', 'dataset', 'dataset_numeric'])],prep_pca_data.loc[prep_pca_data['state']==state,'dataset_numeric'],cv=StratifiedKFold(n_splits=5), scoring=scoring)\n",
    "    print(f\"Results for {state}:\")\n",
    "    for name,values in cv_results_pca.items():\n",
    "        print(name,\":\", np.mean(list(values)))\n",
    "    \n",
    "    labels_pred_proc = cross_val_predict(SVC(gamma='auto'),prep_pca_data.loc[prep_pca_data['state']==state,~prep_pca_data.columns.isin(['state', 'dataset', 'dataset_numeric'])],prep_pca_data.loc[prep_pca_data['state']==state,'dataset_numeric'],cv=StratifiedKFold(n_splits=5))\n",
    "    labels_true_proc = prep_pca_data.loc[prep_pca_data['state']==state,'dataset_numeric']\n",
    "    \n",
    "    fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.clf()\n",
    "    res = sns.heatmap(contingency_matrix(labels_true_proc, labels_pred_proc), fmt='.2f', cmap=\"YlGnBu\", vmin=0.0, vmax=100.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = np.asarray(data.loc[:,~data.columns.isin(['state', 'dataset'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_Y = np.linalg.norm(te[:20000, np.newaxis] - te[:20000,:], axis=-1)\n",
    "plt.matshow(pd_Y, cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def recurrence_plot(data, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Generate a recurrence plot from a time series.\n",
    "\n",
    "    :param data: Time series data\n",
    "    :param threshold: Threshold to determine recurrence\n",
    "    :return: Recurrence plot\n",
    "    \"\"\"\n",
    "    # Calculate the distance matrix\n",
    "    N = len(data)\n",
    "    distance_matrix = np.zeros((N, N))\n",
    "    count = 0\n",
    "    for i in tqdm(range(N)):\n",
    "        for j in range(N):\n",
    "            distance_matrix[i, j] = np.linalg.norm(data[i] - data[j]) # euclidean distance between two points\n",
    "            if distance_matrix[i, j] <= threshold:\n",
    "                count += 1\n",
    "\n",
    "    # Create the recurrence plot\n",
    "    recurrence_plot = np.where(distance_matrix <= threshold, 1, 0)\n",
    "    print(count)\n",
    "    return recurrence_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot the recurrence plot of the first principal component\n",
    "recurrence = recurrence_plot(np.array(data.loc[:6000,0]), threshold=0.8) # run time and memory allocation for full dataset is too high \n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(recurrence, cmap='Greys', origin='lower')\n",
    "plt.title('Recurrence Plot')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Time')\n",
    "plt.colorbar(label='Recurrence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "dist = pdist(te[:10000, :])\n",
    "dist = squareform(dist)\n",
    "sns.heatmap(dist, cmap=\"mako\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = data.groupby('dataset')\n",
    "all_dfs = []\n",
    "for name, group in groups:\n",
    "    df = group.reset_index().loc[:900,:]\n",
    "    all_dfs.append(df)\n",
    "data_truncated = pd.concat(all_dfs)\n",
    "te_trunc = np.asarray(data_truncated.loc[:,~data_truncated.columns.isin(['state', 'dataset'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "dist = pdist(te_trunc)\n",
    "dist = squareform(dist)\n",
    "sns.heatmap(dist, cmap=\"mako\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix I: More Clustering\n",
    "\n",
    "### 5 clusters and comparing with state membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMI_states = cross_val_score(KMeans(n_clusters=5),prep_pca_data.loc[:,~prep_pca_data.columns.isin(['state', 'dataset', 'dataset_numeric'])],prep_pca_data['state'],cv=StratifiedKFold(n_splits=5),scoring=make_scorer(adjusted_mutual_info_score))\n",
    "print(f\"AMI for unpreprocessed data: {np.mean(AMI_states)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is little correspondence between the clusters and the state membership. This suggests that the clustering is not based on the state membership.\n",
    "\n",
    "### 23 clusters but within each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = prep_pca_data['state'].unique().tolist()\n",
    "for state in states:\n",
    "    AMI = cross_val_score(KMeans(n_clusters=23),prep_pca_data.loc[prep_pca_data['state']==state,~prep_pca_data.columns.isin(['state', 'dataset', 'dataset_numeric'])],prep_pca_data['dataset_numeric'],cv=StratifiedKFold(n_splits=5),scoring=make_scorer(adjusted_mutual_info_score))\n",
    "    print(f\"AMI for unpreprocessed data: {np.mean(AMI)}\")\n",
    "    \n",
    "    labels_pred_proc = cross_val_predict(KMeans(n_clusters=23),prep_pca_data.loc[prep_pca_data['state']==state,~prep_pca_data.columns.isin(['state', 'dataset', 'dataset_numeric'])],prep_pca_data['dataset_numeric'],cv=StratifiedKFold(n_splits=5))\n",
    "    labels_true_proc = prep_pca_data.loc[prep_pca_data['state']==state,'dataset_numeric']\n",
    "    \n",
    "    fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.clf()\n",
    "    res = sns.heatmap(contingency_matrix(labels_true_proc, labels_pred_proc), fmt='.2f', cmap=\"YlGnBu\", vmin=0.0, vmax=100.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control\n",
    "states = prep_pca_data['state'].unique().tolist()\n",
    "for state in states:\n",
    "    AMI = cross_val_score(KMeans(n_clusters=23),unprep_pca_data.loc[prep_pca_data['state']==state,~unprep_pca_data.columns.isin(['state', 'dataset', 'dataset_numeric'])],unprep_pca_data['dataset_numeric'],cv=StratifiedKFold(n_splits=5),scoring=make_scorer(adjusted_mutual_info_score))\n",
    "    print(f\"AMI for unpreprocessed data: {np.mean(AMI)}\")\n",
    "    \n",
    "    labels_pred_proc = cross_val_predict(KMeans(n_clusters=23),unprep_pca_data.loc[unprep_pca_data['state']==state,~unprep_pca_data.columns.isin(['state', 'dataset', 'dataset_numeric'])],unprep_pca_data['dataset_numeric'],cv=StratifiedKFold(n_splits=5))\n",
    "    labels_true_proc = unprep_pca_data.loc[unprep_pca_data['state']==state,'dataset_numeric']\n",
    "    \n",
    "    fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.clf()\n",
    "    res = sns.heatmap(contingency_matrix(labels_true_proc, labels_pred_proc), fmt='.2f', cmap=\"YlGnBu\", vmin=0.0, vmax=100.0)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix II: Quantification of variability within state with Median Points\n",
    "\n",
    "We want to perform PCA on the ventral state points and identify the direction (eigenvector) that captures a lot of variance but across different trajectories and not just within a single trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ventral_te = prep_pca_data.copy()\n",
    "turn_vec = prep_pca_data['state'].values\n",
    "dataset_names = []\n",
    "intervals = []\n",
    "is_ventral = False\n",
    "count = 0\n",
    "for i in range(ventral_te.shape[0]):\n",
    "    if turn_vec[i] == 'ventral':\n",
    "        if is_ventral:\n",
    "            continue\n",
    "        else:\n",
    "            start_idx=i\n",
    "            is_ventral = True\n",
    "        continue\n",
    "    else:\n",
    "        if not is_ventral:\n",
    "            continue\n",
    "        else:\n",
    "            end_idx=i-1\n",
    "            array = ventral_te.loc[start_idx:end_idx,~ventral_te.columns.isin(['dataset'])]\n",
    "            dataset_names.append(ventral_te['dataset'].loc[start_idx])\n",
    "            intervals.append(array)\n",
    "            is_ventral = False\n",
    "all_medians = []\n",
    "for i in intervals:\n",
    "    quan = i.loc[int(np.median(i.index, axis=0)), :]\n",
    "    all_medians.append(quan)\n",
    "    \n",
    "pca_median = PCA(n_components=3)\n",
    "median_pcs = pca_median.fit_transform(pd.DataFrame(all_medians))\n",
    "median_pc1 = median_pcs[:,0]\n",
    "median_comps = pca_median.components_\n",
    "#med = list(np.concatenate(median_pcs))\n",
    "med_df = pd.DataFrame(median_pc1, columns=['Median'])\n",
    "med_df[\"dataset\"] = dataset_names\n",
    "med_df.head()\n",
    "sns.histplot(data=med_df, x=\"Median\",y=\"dataset\",hue='dataset',bins=250, legend=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(median_pc1, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vtp = pd.DataFrame(ventral_te_pc)\n",
    "vtp['state'] = turn_vec\n",
    "\n",
    "fig = hf.plot_PCs(vtp)\n",
    "\n",
    "for i in range(len(all_medians)):\n",
    "    fig.add_trace(go.Scatter3d(x=[all_medians[i][0]],\n",
    "                                y=[all_medians[i][1]],\n",
    "                                z=[all_medians[i][2]],\n",
    "                                mode='markers',\n",
    "                                marker=dict(color='black', size=3)))\n",
    "    \n",
    "# scale the components\n",
    "scaled_ventral_components = np.zeros(median_comps.shape)\n",
    "max_coord = np.abs(ventral_te_pc).max(axis=1).max()\n",
    "scaled_ventral_components[0]=median_comps[0]*max_coord\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=[-scaled_ventral_components[0, 0], scaled_ventral_components[0, 0]],\n",
    "                            y=[-scaled_ventral_components[0, 1], scaled_ventral_components[0, 1]],\n",
    "                            z=[-scaled_ventral_components[0, 2], scaled_ventral_components[0, 2]],\n",
    "                            mode='lines', name=f'Principal Component 1',\n",
    "                            line=dict(color='black', width=3)))\n",
    "    \n",
    "fig.update_xaxes(type='linear')\n",
    "fig.update_yaxes(type='linear')\n",
    "fig.update_layout(title='PCA of time-embedded data')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
