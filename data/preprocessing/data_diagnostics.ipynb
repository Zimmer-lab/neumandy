{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Data Diagnostics I </b> *✲ﾟ*｡✧٩(･ิᴗ･ิ๑)۶*✲ﾟ*｡✧\n",
    "\n",
    "In this notebook we will explore taking the min-max or percentile normalization between datasets and also derivatives and see how our data changes, i.e. the distribution of each variable, check the principal components, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions as hf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "imputed_dataframe = pd.read_hdf(\"imputed_dataframe_0602.h5\")\n",
    "turn_vec = imputed_dataframe[\"state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.px.scatter(imputed_dataframe, y=\"SMDVR\", color=imputed_dataframe[\"state\"], color_continuous_scale='viridis', marginal_y='histogram').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behaviour state annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the behaviour state annotations to discrete values\n",
    "dataframe = imputed_dataframe.copy()\n",
    "#turn_vec = imputed_dataframe[\"state\"]\n",
    "\n",
    "conditions = [\n",
    "    ((turn_vec) == 1.0),\n",
    "    ((turn_vec)  == 2.0),\n",
    "    ((turn_vec)  == 3.0),\n",
    "    ((turn_vec)  == 4.0)\n",
    "]\n",
    "\n",
    "values = ['forward', 'reversal', 'sustained reversal', 'turn']\n",
    "\n",
    "# replace values based on conditions, 1 = forward, 2 = reversal (rise), 3 = reversal (sustained), 4 = turn\n",
    "transformed_turn_vec = np.select(conditions, values)\n",
    "\n",
    "dataframe[\"state\"] = hf.determine_turn(dataframe, transformed_turn_vec) # to determine whether a turn is a dorsal or a ventral turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.px.scatter(dataframe, y=\"AVAR\", color=dataframe[\"state\"], color_continuous_scale='viridis', marginal_y='histogram').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling (or Up-/Downsampling)\n",
    "Our datasets have different sizes, so we have to upsample them. Most recordings range from 3200 to 3780 time points as we can see in the below figure but there is one dataset with 4146 and one with 5450 time points. 8 datasets have exactly 3529 time points. We will therefore down- or upsample to this number via linear interpolation (computing the slope between two data points) implemented in numpy.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.visualize_fps(dataframe, title=\"frame rate of each dataset\", xlabel=\"dataset\", ylabel=\"frame rate\", coloring=\"tab:red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_num = 3529\n",
    "\n",
    "# resample all dataframes to the same length of 3529 frames\n",
    "resampled_dataframe = hf.resample(dataframe, list(dataframe.groupby('dataset').size().values), frames_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.px.scatter(resampled_dataframe, y=\"PVR\", color=resampled_dataframe[\"state\"], color_continuous_scale='viridis', marginal_y='histogram').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation\n",
    "\n",
    "We noticed some edge effects in the data, i.e. the first and last 100 time points are not very reliable. We will therefore truncate the data to the middle 3329 time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncating with a default n of 100, i.e. we remove the first 100 and the last 100 observations from each dataset\n",
    "truncated_dataframe = hf.truncate(resampled_dataframe, n=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_dataframe.to_hdf(\"truncated_dataframe_0602.h5\", key=\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization between datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above we have to deal with different scales across datasets so a natural next step is to normalize the data across datasets to make them comparable. We will do this by taking the min-max normalization between datasets. This means that we will take the minimum and maximum value of each variable across all datasets and then normalize each dataset to this range. This will be done the time derivatives of the resampled data.\n",
    "\n",
    "We can also try the percentile normalization between datasets. This means that we will take the 5th and 95th percentile of each variable across all datasets and then normalize each dataset to this range. This will be done the time derivatives of the resampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Quantiles: RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler(with_centering=False, with_scaling=True, quantile_range=(5, 99))\n",
    "scaler2 = RobustScaler(with_centering=False, with_scaling=True, quantile_range=(5, 99), unit_variance=True)\n",
    "\n",
    "# normalize per dataset\n",
    "quartiled_separate = hf.normalize_per_dataset(truncated_dataframe.loc[:, ~truncated_dataframe.columns.isin([\"state\", \"dataset\"])], truncated_dataframe.groupby(\"dataset\").size().values, scaler)\n",
    "\n",
    "# normalize across datasets \n",
    "quartiled_data = pd.DataFrame(scaler2.fit_transform(quartiled_separate), columns = quartiled_separate.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiled_data[\"state\"] = truncated_dataframe[\"state\"]\n",
    "quartiled_data[\"dataset\"] = truncated_dataframe[\"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"SMD\"\n",
    "hf.px.scatter(quartiled_data, y=column, color=quartiled_data[\"state\"], color_continuous_scale='viridis', marginal_y='histogram',hover_name=\"state\", hover_data=[\"dataset\", quartiled_data.index, column]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.px.scatter(quartiled_data, y=\"PVR\", color=\"state\", color_continuous_scale='viridis', marginal_y='histogram',hover_name=\"dataset\", hover_data=[\"state\", quartiled_data.index, column]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization with 20% quantile subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def quantile_subtract(X):\n",
    "    for column in X.loc[:,~X.columns.isin([\"state\",\"dataset\"])].columns:\n",
    "        quantile_20 = X[column].quantile(0.20)\n",
    "        X[column] = X[column].apply(lambda x: x - quantile_20)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiled_data_copy = quartiled_data.copy()\n",
    "quartiled_data_copy = quartiled_data_copy.groupby(\"dataset\").apply(lambda x: quantile_subtract(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.px.scatter(quartiled_data_copy, y=\"AVAR\", color=\"state\", color_continuous_scale='viridis', marginal_y='histogram',hover_name=\"dataset\", hover_data=[\"state\", quartiled_data.index, column]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiled_data_copy.to_hdf(\"quartiled_data_0602.h5\", key=\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiled_data_copy.to_hdf(\"quartiled_data_0602.h5\", key=\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_quartile = hf.PCA(n_components=3)\n",
    "imputed_pc_quartile = pd.DataFrame(pca_quartile.fit_transform(quartiled_data_copy.loc[:, ~quartiled_data_copy.columns.isin([\"state\",'dataset'])]))\n",
    "\n",
    "window_size = 10\n",
    "\n",
    "# Applying a 10-sample sliding average for smoother visualizations!\n",
    "imputed_pc_quartile[0] = np.convolve(imputed_pc_quartile[0], np.ones(window_size)/window_size, mode='same')\n",
    "imputed_pc_quartile[1] = np.convolve(imputed_pc_quartile[1], np.ones(window_size)/window_size, mode='same')\n",
    "imputed_pc_quartile[2] = np.convolve(imputed_pc_quartile[2], np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "imputed_pc_quartile['state'] = truncated_dataframe[\"state\"]\n",
    "hf.plot_PCs(imputed_pc_quartile,imputed_pc_quartile['state'],'PCA_quartiled_0202.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix I: PCA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiled_transposed_dataframe = quartiled_data_copy.loc[:,~quartiled_data_copy.columns.isin([\"state\",\"dataset\"])].T\n",
    "n_components = 3\n",
    "\n",
    "pca_all_splits = hf.get_LLO_PCAs(quartiled_transposed_dataframe, n_components=n_components)\n",
    "\n",
    "fig = hf.make_subplots(rows=1, cols=1, shared_xaxes=True, y_title= \"PCA weights\", vertical_spacing=0.05)\n",
    "\n",
    "variable_name = f\"pca3_all_splits\"\n",
    "concatenated_pca = hf.pd.concat(pca_all_splits[variable_name], axis=0)\n",
    "concatenated_pca.sort_values(by=['Mode 3'], inplace=True)\n",
    "fig.add_trace(hf.go.Box(\n",
    "    x=concatenated_pca['neuron'],\n",
    "    y=concatenated_pca['Mode 3'],\n",
    "    boxpoints=False,\n",
    "    name='Mode 3'\n",
    "), row=1, col=1)\n",
    "\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"First 3 PC weights for all neurons\",\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "fig.write_html(\"PCA_neuron_weights.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix II: Grid Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# we plot all the resampled traces \n",
    "saving_path=\"..\\\\plots\\\\23Jan\\\\resampled_plots\\\\\"\n",
    "\n",
    "hf.plot_from_stacked_imputed(length_dict, resampled_dataframe, resampled_dataframe, saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# we plot all the resampled traces \n",
    "saving_path=\"..\\\\plots\\\\23Jan\\\\truncated_plots\\\\\"\n",
    "\n",
    "hf.plot_from_stacked_imputed(length_dict, truncated_dataframe, truncated_dataframe, saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for column in quartiled_data_copy.columns:\n",
    "    # plotting the trace of one neuron across all datasets\n",
    "    # and save the plot\n",
    "    fig, ax = plt.subplots(figsize=(40, 10))\n",
    "    ax.plot(quartiled_data_copy[column].T, color=\"tab:blue\")\n",
    "    ax.set_ylabel(column)\n",
    "    ax.set_xlabel(\"time\")\n",
    "    ax.set_title(column+\"across all datasets\")\n",
    "    fig.savefig(\"..\\\\plots\\\\06Feb\\\\all_traces_normalized\\\\normalized_\"+column+\"_alldatasets.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib widget\n",
    "saving_path=\"..\\\\plots\\\\23Jan\\\\normalized_5_95\\\\\"\n",
    "\n",
    "hf.plot_from_stacked_imputed(length_dict, quartiled_data, quartiled_data, saving_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib widget\n",
    "saving_path=\"..\\\\plots\\\\23Jan\\\\normalized_on_truncated\\\\\"\n",
    "\n",
    "hf.plot_from_stacked_imputed(length_dict, quartiled_data, truncated_dataframe, saving_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
