{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Data Exploration </b> *✲ﾟ*｡✧٩(･ิᴗ･ิ๑)۶*✲ﾟ*｡✧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "<b>What data?</b> <br> We are working with the immobilized whole brain imaging data from Kerem and Rebecca. They only include control data, meaning that the worm is a wild type and has no modification apart from the GFP. The data, which were originally stored in separate wbstruct.mat files, have been converted to a dictionary of pandas dataframes (stored in a pickle file for easier handling but also into separate h5 files and csv files for easier sharing). \n",
    "\n",
    "<b>What kind of exploration?</b> <br> \n",
    "We want to understand the individual datasets and get a feeling of what problems we might face, whether we need to do some processing before we start with the analysis. For now, we especially want to know and make following decisions:\n",
    "- Are all neurons in each dataset IDed? (If no, remove)\n",
    "- How many neurons are IDed in each dataset?\n",
    "- How many times is each neuron IDed in total? (If too few, remove or impute)\n",
    "- Are neurons with few total number of IDs unique? (If no, impute. Else remove)\n",
    "- How to deal with missing IDs of nonunique neurons? Which imputation method? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions as hf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Quantifications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will first look at the number of IDs per neuron in all datasets and the number of IDs per dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe dictionaries\n",
    "dataframes=hf.wbstruct_dataframes.loading_pkl('dataframes_rebecca_2301.pkl')\n",
    "dataframes_kerem=hf.wbstruct_dataframes.loading_pkl('dataframes_kerem_0602.pkl') # data loaded with wbstruct_converter\n",
    "\n",
    "# merging rebecas and kerems dataframe dictionaries\n",
    "dataframes.update(dataframes_kerem)\n",
    "hf.wbstruct_dataframes.saving_as_pkl(dataframes, 'dataframes_0602.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate state annotations\n",
    "annotations = []\n",
    "for k,v in dataframes.items():\n",
    "   annotations.append(v[\"state\"].values)\n",
    "   v.drop(\"state\", axis=1, inplace=True)\n",
    "   dataframes[k] = v\n",
    "annotations = np.concatenate(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_dataframe = hf.pd.concat([df for df in dataframes.values()], ignore_index=True)\n",
    "threshold = 10\n",
    "\n",
    "# we stack all neurons that have less than 10 IDs\n",
    "all_IDed_neurons, IDs_per_set = hf.count_IDs(dataframes) # count how many times each neuron was IDed\n",
    "stacked_dataframe = stacked_dataframe.drop(columns=[neuron for neuron in stacked_dataframe.columns if all_IDed_neurons[neuron] < threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding which dataset each observation belongs to\n",
    "new_col = []\n",
    "for key, value in dataframes.items():\n",
    "    new_col.extend([key] * len(value))\n",
    "stacked_dataframe['dataset'] = new_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's visualize this information with a cute matplotlib plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.visualize_IDs(IDs_per_set, title=\"Plot of datasets and number of IDs\", xlabel=\"dataset\", ylabel=\"IDs\", coloring=\"tab:green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of IDed neurons per set\n",
    "fig, ax = hf.visualize_IDs(all_IDed_neurons, title=\"Plot of names of neurons and number of times IDed\",xlabel=\"neuron ID\",ylabel=\"count in all datasets\",display_all_values=True)\n",
    "hf.plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most neurons are ID'ed more than 16 times while there are some neurons that are IDed less than 10 times. Considering we have 25 datasets, having to impute more IDs that exist in all datasets might not be a good idea. We could set a threshold to only impute IDs that are present in at least 10 datasets. This would reduce the number of IDs to impute to 25.\n",
    "But before we make the cut we want to know how unique each neuron is. Let's say neuron RMER, which isn't IDed very much, is not very unique in the entirety of the datasets, meaning we could impute the recording of the neuron in all the datasets where it's missing. But if RMER is very unique, the imputation might lead to a completely wrong result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ID imputation with PPCA\n",
    "\n",
    "Probabilistic PCA aims to estimate the principal axes of a datset through maximum likelihood estimation of parameters in a latent variable model. It is a probabilistic formulation of PCA that is more numerically stable and allows for missing data. With PPCA we can impute missing neuronal activities in some of our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_dataframe_copy = stacked_dataframe.copy()\n",
    "stacked_dataframe_copy = stacked_dataframe_copy.drop(columns=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is imputed with PPCA\n",
    "imputed_dataframe_og = hf.utils_imputation.impute_missing_values_in_dataframe(stacked_dataframe_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_dataframe_og[\"state\"] = annotations\n",
    "imputed_dataframe_og[\"dataset\"] = stacked_dataframe[\"dataset\"]\n",
    "imputed_dataframe_og.to_hdf(\"imputed_dataframe_0602.h5\", key=\"imputed_dataframe_0602\")\n",
    "imputed_dataframe_og.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix I: \"Uniqueness\" of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to know two things: how unique is each neuron across all datasets and does it make sense to use PPCA for data imputation considering that it assumes that the variables can be linearly modelled. \n",
    "We will run a Least Square Regression model d(=number of datasets)*n(=number of neurons) times where in each round y will be a single neuron and X will be all neurons but y. The aim is to see how well a neuron can be explained by all the other neurons. We can get a rough understanding of this by looking at the R<sup>2</sup> value of each LS model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Squared R<sup>2</sup>\n",
    "The R<sup>2</sup> measures the proportion of the neurons variance or spread explained by all the other neurons. R<sup>2</sup> ranges from 0 to 1 where 1 indicates that all the variance is explained by the other neurons and 0 indicates that none of the variance is explained by the other neurons. Now, if in our case the R<sup>2</sup>-value is high, we can say that the neuron is not very unique and we can impute the missing IDs. If the R<sup>2</sup>-value is low, we can say that the neuron is very unique and we should not impute the missing IDs. We will look at the average R<sup>2</sup> across all datasets. <br>\n",
    "Runtime: ~ 3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_r2, predictions, importances, raw_data = hf.get_R2_predictions(dataframes, all_IDed_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.visualize_IDs(avg_r2, title=\"Average R2 values\", xlabel=\"neuron ID\", ylabel=\"R2\", coloring=\"tab:blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = min(avg_r2.values(), key=lambda x: abs(x - 0.7)) # this finds the closest R2 value to 0.7\n",
    "\n",
    "percent = hf.find_percent(avg_r2.values(), min_value) # this finds the percentage of neurons that have an average R2 value of at least 0.7\n",
    "print(\"{:.2f}% of neurons have an average R2 value of at least {:.2f}\".format(percent, min_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that more than 67 percent of neurons have an average R<sup>2</sup>-value of at least 0.7, we can say that most neurons are not very unique. This means that we can impute the missing IDs of most neurons and use PPCA for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix II: Variable p-values of the linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know which neurons were important for modelling some of the neurons in the datasets. We will look at the p-values of each linear regression model. The p-value indicates the significance of each variable during the modelling of each neuron.\n",
    "\n",
    "Calculation of p-value: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take the mean VIP score of each neuron, sort them and take the top 5\n",
    "for key,value in importances.items():\n",
    "    for neuron, list in value.items():\n",
    "        importances[key][neuron] = hf.np.mean(list)\n",
    "    importances[key] = sorted(importances[key].items(), key=lambda item: item[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plot the top 5 neurons with the highest variable importances per neuron\n",
    "fig, axes = plt.subplots(5, 6, figsize=(18, 9), sharey=True)\n",
    "fig.suptitle('Top 5 neurons with highest Variable Importances per neuron')\n",
    "plt.subplots_adjust(hspace = 2)\n",
    "axes = axes.flatten()\n",
    "count = 0\n",
    "palette = iter(sns.husl_palette(30))\n",
    "for i in importances.keys():\n",
    "    \n",
    "    keys = [neuron[0] for neuron in importances[i]]\n",
    "    vals = [neuron[1] for neuron in importances[i]]\n",
    "    sns.barplot(ax = axes[count], x=keys, y=vals, dodge=False, color=next(palette))\n",
    "    axes[count].set_title(i)\n",
    "    count = count + 1\n",
    "    \n",
    "    # cut off the plot after 30 neurons so that the plot is not too big\n",
    "    if count % round(len(importances.keys())/2.5) == 0:\n",
    "        plt.show()\n",
    "        fig, axes = plt.subplots(5, 6, figsize=(18, 9), sharey=True)\n",
    "        fig.suptitle('Top 5 neurons with highest Variable Importances per neuron')\n",
    "        plt.subplots_adjust(hspace = 2)\n",
    "        axes = axes.flatten()\n",
    "        count = 0\n",
    "        palette = iter(sns.husl_palette(30))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix III: Saving plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelled neuron activity against true activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib widget\n",
    "# we don't want to output all plots, just save them\n",
    "\n",
    "delta_path=\"..\\\\plots\\\\23Jan\\\\delta_plots\\\\\"\n",
    "model_path=\"..\\\\plots\\\\23Jan\\\\modelled_plots\\\\\"\n",
    "\n",
    "plot_kwargs = {'alpha': 0.7}\n",
    "\n",
    "hf.plot_from_single_imputed(raw_data, predictions, delta_path, model_path, plot_kwargs=plot_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputed neuron activity against existing true activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# we will save all dataframe keys and their lengths in a dictionary for the unstacking part\n",
    "length_dict = defaultdict()\n",
    "for key, value in dataframes.items():\n",
    "    length_dict[key] = len(value)\n",
    "\n",
    "saving_path=\"..\\\\plots\\\\23Jan\\\\imputed_plots\\\\\"\n",
    "\n",
    "hf.plot_from_stacked_imputed(length_dict, imputed_dataframe, stacked_dataframe, saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6b8062c8d873bf17e80439cf9ee27761532659cc2634ffd2b6294fa54795af1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
