{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6f0068-5222-4cdd-82bc-c2d5c0751466",
   "metadata": {},
   "source": [
    "# Converting MATLAB wbstructs to pandas dataframes\n",
    "\n",
    "Check the README.md for more information when to use this tool. This notebook simply calls the functions in the wbstruct_converter.py file. To get a better understanding of the code, please refer to the wbstruct_converter.py file. If there are unhandled errors, please open an issue on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8a2764-c12f-48c7-a1a2-e7095b44738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.wbstruct_to_dicts as wbstruct_dictioniaries\n",
    "import utils.wbstruct_dicts_to_dataframes as wbstruct_dataframes\n",
    "import dill\n",
    "\n",
    "# in case we want to reload our module\n",
    "import importlib\n",
    "importlib.reload(wbstruct_dictioniaries)\n",
    "importlib.reload(wbstruct_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9584c141-073b-489e-8ddb-77a69845ddd8",
   "metadata": {},
   "source": [
    "### Convert the wbstructs to python dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c861f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining directory paths where all recordings are located, first is from Rebecca and second from Kerem Uzel\n",
    "directory_paths = [\"Y:\\\\lisc\\\\project\\\\neurobiology\\\\zimmer\\\\Rebecca\",\n",
    "                   \"Y:\\\\lisc\\\\project\\\\neurobiology\\\\zimmer\\\\Kerem_Uzel\\\\Whole_brain_imaging\\\\Cleaned_up_datasets\\\\WT\\\\ItamarCorrected_OLQ_URY_2023\\\\\"]\n",
    "\n",
    "# defining target file name which should be wbstruct.mat since this is the file from the wba that we want to convert into a DF\n",
    "target_file = \"wbstruct.mat\"\n",
    "\n",
    "# since we usually have multiple recordings per animal we want to include only those that are relevant for our analysis\n",
    "# following is specific to Rebecca's and Kerem's Data\n",
    "include_Rebecca=[\"Ctrl\",\"used Datasets\"]\n",
    "include_Kerem=[\"Head\",\"Tail\"]\n",
    "exclude=[\"not_used\",\"not used\",\"notUsed\",\"cat-2_tdc-1_tph-1\"]\n",
    "recording_type=\"deltaFOverF_bc\"\n",
    "simple=False\n",
    "save_as='csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and save kerem's data\n",
    "datasets_kerem = wbstruct_dictioniaries.get_datasets_dict(directory_paths[1],target_file, include_Kerem,exclude,recording_type, simple=True, with_coloring=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b22ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_kerem.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade514f0",
   "metadata": {},
   "source": [
    "The dictionary returned by the function is also saved as a pickle file named \"datasets.pkl\". In order to load this file, perform following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fefe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load kerem's dictionary\n",
    "with open('datasets.pkl','rb') as f:\n",
    "    datasets_kerem = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f6938",
   "metadata": {},
   "source": [
    "Now we can use the dictionaries as they are or convert them to pandas dataframes.\n",
    "\n",
    "### Convert dictionaries to dataframes\n",
    "For the following function **get_dataframes()** we assume that the \"ID1\" contains all IDs (non-IDed objects have the ID \"None\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abff407",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_kerem = wbstruct_dataframes.get_dataframes(datasets_kerem, recording_type, with_2_traces=True, with_coloring=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73339662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load kerem's dictionary\n",
    "with open('../preprocessing/dataframes_kerem_0602.pkl','wb') as f:\n",
    "    dill.dump(dataframes_kerem,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available Recordings:\",list(dataframes_kerem.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672e84c-42f4-4292-a1bc-28a691a24342",
   "metadata": {},
   "source": [
    "### If the IDs are in separate Excel Sheets [OLD]\n",
    "In case the IDs of the neurons are stored in separate Excel Sheets. To construct the final dataframe we have to merge the collected IDs with the datasets from the dictionary. This step is old, since a new data struct containing the values but also the IDs and the behaviour state annotation was discovered.\n",
    "I keep this section here still because we might need it if we want to play around with the raw F traces rather than the bleach corrected and the deltaF/F traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following directory contains all the excel files that hold the information about the recordings\n",
    "directory_path_ID = \"Y:\\\\lisc\\\\project\\\\neurobiology\\\\zimmer\\\\Rebecca\\\\Analyses\"\n",
    "target_ID = \".xlsx\"\n",
    "include_ID = [\"Analyses_\"]\n",
    "exclude_ID = [\"._\",\"cat-2_tdc-1_tph-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18258a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictofIDs_og = wbstruct_dictioniaries.get_IDs_dict(directory_path_ID, target_ID, include_ID, exclude_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525e512-ef58-42d9-b06e-cdba929c9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_rebecca=wbstruct_dataframes.get_dataframes_from_excel(datasets_rebecca, dictofIDs_og, recording_type, save_as='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb62a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available Recordings:\",list(dataframes_rebecca.keys()))\n",
    "dataframes_rebecca['20200629_w1'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38c367",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6b8062c8d873bf17e80439cf9ee27761532659cc2634ffd2b6294fa54795af1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
